{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import doubleml as dml\n",
    "import patsy\n",
    "\n",
    "# Define random state\n",
    "np.random.seed(123)\n",
    "rs = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import fullly preprocessed dataset\n",
    "filepath = \"../data_processing/processed_merged_data/full_dataset.csv\"\n",
    "df = pd.read_csv(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log transform GVA_Real_Density and Tax_D_Real\n",
    "df['GVA_Real_Density'] = np.log(df['GVA_Real_Density'])\n",
    "df['Tax_D_Real'] = np.log(df['Tax_D_Real'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cutoff values based on the propensity score analysis\n",
    "tax_cutoff = 7.2 # Justify with generalised propensity score\n",
    "gva_cutoff = 11 # Justify with scatter plot\n",
    "\n",
    "# Find LANMs that are outside of range consistently\n",
    "outliers = df.groupby('LANM').filter(\n",
    "    lambda x: x['Tax_D_Real'].mean() < tax_cutoff and \n",
    "    x['GVA_Real_Density'].mean() > gva_cutoff\n",
    ")\n",
    "\n",
    "outliers = outliers['LANM'].unique()\n",
    "print(f\"List of Outlier LAs: {outliers}\")\n",
    "\n",
    "# Remove outliers from the dataset\n",
    "df = df[~df['LANM'].isin(outliers)]\n",
    "\n",
    "# Calculate total missing rows\n",
    "num_years = df['Year'].nunique()\n",
    "dropped_rows = len(outliers) * num_years\n",
    "print(f\"Number of rows dropped: {dropped_rows}\")\n",
    "print(f\"Rows of data remaining: {df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove alternate outcome \"GVA_Real\"\n",
    "ml_df = df.drop(columns=['GVA_Real'])\n",
    "\n",
    "# Remove all cols with GDHI in the name (mediators)\n",
    "ml_df = ml_df.loc[:, ~ml_df.columns.str.contains('GDHI')]\n",
    "\n",
    "# Remove non time-varying variables\n",
    "ml_df = ml_df.drop(columns=['Area', 'Region', 'Authority_Type'])\n",
    "\n",
    "# Remove id name columns\n",
    "ml_df = ml_df.drop(columns=['LANM'])\n",
    "\n",
    "# Duplicate year column\n",
    "ml_df['Year_Group'] = ml_df['Year'].astype('str')\n",
    "\n",
    "# Duplicate LACD column\n",
    "ml_df['LACD_Group'] = ml_df['LACD'].astype('str')\n",
    "\n",
    "# Create one hot encoding for fixed effects\n",
    "ml_df = pd.get_dummies(ml_df, columns=['LACD'])\n",
    "\n",
    "# Show columns and number of columns\n",
    "print(ml_df.columns)\n",
    "print(len(ml_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Nuisance Parameter Learners\n",
    "ml_l = GradientBoostingRegressor(random_state=rs)\n",
    "ml_m = GradientBoostingRegressor(random_state=rs)\n",
    "\n",
    "# Define two-dimensional clustering\n",
    "clusters = ['LACD_Group', 'Year_Group']\n",
    "\n",
    "# Create DoubleML cluster data object\n",
    "obj_dml_data = dml.DoubleMLClusterData(ml_df,\n",
    "                                       'GVA_Real_Density',\n",
    "                                       'Tax_D_Real',\n",
    "                                       clusters)\n",
    "\n",
    "# Create DoubleML Partially Linear Regression object\n",
    "dml_plr_obj = dml.DoubleMLPLR(obj_dml_data, ml_l, ml_m)\n",
    "\n",
    "# Define parameter grids for tuning\n",
    "par_grids = {'ml_l': {'n_estimators': [300, 400, 600, 800],\n",
    "                      'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "                      'max_features': [0.2, 0.5, 0.8],\n",
    "                      'max_depth': [3, 5, 10, 30],\n",
    "                      'min_samples_split': [2, 4, 8],\n",
    "                      'min_samples_leaf': [1, 2, 5]},\n",
    "             'ml_m': {'n_estimators': [400, 600, 800, 1000],\n",
    "                      'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "                      'max_features': [0.2, 0.5, 0.8],\n",
    "                      'max_depth': [3, 5, 10, 30],\n",
    "                      'min_samples_split': [2, 4, 8],\n",
    "                      'min_samples_leaf': [1, 2, 5]}}\n",
    "\n",
    "# Tune hyperparameters\n",
    "dml_plr_obj.tune(par_grids, search_mode='randomized_search',\n",
    "                 n_iter_randomized_search=50)\n",
    "\n",
    "# Print optimal hyperparameters\n",
    "print(dml_plr_obj.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit DML model\n",
    "print(dml_plr_obj.fit())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
